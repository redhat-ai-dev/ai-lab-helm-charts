apiVersion: apps/v1
kind: Deployment
metadata:
  labels: 
    app.kubernetes.io/instance: {{ .Values.application.name }}-model-server
    app.kubernetes.io/name: {{ .Values.application.name }}-model-server
    app.kubernetes.io/part-of: {{ .Values.application.name }}  
  name: {{ .Values.application.name }}-model-server
  namespace: {{ .Values.application.namespace }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: {{ .Values.application.name }}-model-server 
  template:
    metadata: 
      labels:
        app.kubernetes.io/instance: {{ .Values.application.name }}-model-server
    spec:
      {{ if eq .Values.model.modelServiceType "llama.cpp" }}
      initContainers:
      - name: model-file
        image: {{ .Values.model.initContainer }}
        command: {{ .Values.model.modelInitCommand }}
        volumeMounts:
        - name: model-file
          mountPath: /shared
      {{ end }}
      containers:
      {{ if eq .Values.model.modelServiceType "vllm" }}
      - image: {{ .Values.model.vllmServiceContainer }}
        args: [
            "--model",
            "{{ .Values.model.modelName }}",
            "--port",
            "{{ .Values.model.modelServicePort }}",
            "--download-dir",
            "/models-cache",
            "--max-model-len",
            "{{ .Values.model.modelMaxLength }}"]
        resources:
          limits:
            nvidia.com/gpu: '1'
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: models-cache
          mountPath: /models-cache
      {{ else }}
      - env:
        - name: HOST
          value: "0.0.0.0"
        - name: PORT
          value: "{{ .Values.model.modelServicePort }}"
        - name: MODEL_PATH
          value: {{ .Values.model.modelPath }}
        - name: CHAT_FORMAT
          value: openchat
        image: {{ .Values.model.modelServiceContainer }}
        name: model-service
        volumeMounts:
        - name: model-file
          mountPath: /model
      {{ end }}
        name: app-model-service
        ports:
        - containerPort: {{ .Values.model.modelServicePort }}
        securityContext:
          runAsNonRoot: true
      {{ if eq .Values.model.modelServiceType "vllm" }}
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: "2Gi"
      - name: models-cache
        persistentVolumeClaim:
          claimName: {{ .Values.application.name }}

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      {{ else }}
      volumes:
      - name: model-file
        emptyDir: {}
      {{ end }}