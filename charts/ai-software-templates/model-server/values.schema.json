{
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "properties": {
        "model": {
            "properties": {
                "initContainer": {
                    "default": "quay.io/redhat-ai-dev/granite-7b-lab:latest",
                    "description": "The image used for the initContainer of the model service deployment.",
                    "title": "Init Container",
                    "type": "string"
                },
                "maxModelLength": {
                    "default": 4096,
                    "description": "The maximum sequence length of the model. It is used only for the vllm case and the default value is 4096.",
                    "title": "Model Max Length",
                    "type": "integer"
                },
                "modelInitCommand": {
                    "default": "['/usr/bin/install', '/model/model.file', '/shared/']",
                    "description": "The model service initContainer command.",
                    "title": "Init Command",
                    "type": "string"
                },
                "modelName": {
                    "description": "The name of the model. It is used only in the vLLM use case.",
                    "title": "Model Name",
                    "type": "string"
                },
                "modelPath": {
                    "default": "/model/model.file",
                    "description": "The path of the model file inside the model service container.",
                    "title": "Path",
                    "type": "string"
                },
                "modelServiceContainer": {
                    "default": "quay.io/ai-lab/llamacpp_python:latest",
                    "description": "The image used for the model service.",
                    "title": "Service Container",
                    "type": "string"
                },
                "modelServicePort": {
                    "default": 8001,
                    "description": "The exposed port of the model service.",
                    "title": "Service Port",
                    "type": "integer"
                },
                "vllmModelServiceContainer": {
                    "description": "The image used for the model service for the vLLM use case.",
                    "title": "vLLM Image",
                    "type": "string"
                },
                "vllmSelected": {
                    "default": false,
                    "description": "Adds support of vLLM instead of llama_cpp. Be sure that your system has GPU support for this case.",
                    "title": "Use vLLM",
                    "type": "boolean"
                }
            },
            "type": "object"
        }
    },
    "title": "Chatbot AI Sample Helm Chart",
    "type": "object"
}